from langchain_core.runnables import RunnableLambda
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from postprocess.config import RANDOM_QUERIES
from langchain import PromptTemplate
from langchain_huggingface import HuggingFacePipeline
from transformers import pipeline
import random
import re
import numpy as np

def create_gen_llm_chain(model, tokenizer, device):
    """
    - HuggingFace pipelineê³¼ PromptTemplateì„ ì¡°í•©í•´ ìƒì„±ìš© LangChain LLMChain ìƒì„±
    - rag_context, query, group_descriptionì„ ì…ë ¥ê°’ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” êµ¬ì¡°
    """
    
    template = """
    ì•„ë˜ëŠ” ê¸°ì¡´ì˜ ë§ˆë‹ˆë˜ ë¯¸ì…˜ ì˜ˆì‹œì•¼:
    {rag_context}

    ìœ„ ì˜ˆì‹œì™€ ì‚¬ìš©ì ì§ˆë¬¸ '{query}', ê·¸ë¦¬ê³  ë§ˆë‹ˆë˜ ê²Œì„ ìˆ˜í–‰ ê·¸ë£¹ì˜ ì„¤ëª… '{group_description}'ì— ì–´ìš¸ë¦¬ëŠ” '{difficulty}' ë‚œì´ë„ì˜ ë§ˆë‹ˆë˜ ë¯¸ì…˜ 5ê°œë¥¼ ì‘ì„±í•´ì¤˜.

    ì¡°ê±´:
    - ê° ë¯¸ì…˜ì€ 'ë¯¸ì…˜ 1:', 'ë¯¸ì…˜ 2:' ë“±ìœ¼ë¡œ ì‹œì‘í•´.
    - ê° ë¯¸ì…˜ì€ ë°˜ë“œì‹œ '~ê¸°'ë¡œ ëë‚˜ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´.
    - êµ¬ì²´ì  ê³ ìœ ëª…ì‚¬ë‚˜ ì¥ì†Œëª…ì€ í¬í•¨í•˜ì§€ ë§ˆ.
    - ì¶œë ¥ì€ ë²ˆí˜¸ê°€ ë¶™ì€ 4ê°œì˜ ë¬¸ì¥ë§Œ ì‘ì„±í•´. ë‹¤ë¥¸ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ.

    ë¯¸ì…˜:
    """

    prompt = PromptTemplate.from_template(template)

    gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=device,
        max_new_tokens=120,
        temperature=0.4,
        top_p=0.8,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        batch_size=4
    )

    gen_llm = HuggingFacePipeline(pipeline=gen_pipe)

    # LangChain LLMChain: prompt | llm êµ¬ì¡°
    gen_llm_chain = prompt | gen_llm
    return gen_llm_chain
  
def create_eval_llm_chain(model, tokenizer, device):
    """
    - HuggingFace pipelineê³¼ PromptTemplateì„ ì¡°í•©í•´ í‰ê°€ìš© LangChain LLMChain ìƒì„±
    - query, missionì„ ì…ë ¥ê°’ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” êµ¬ì¡°
    """
    
    template = """
    ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì½˜í…ì¸  í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì¡°ê±´ì— ë”°ë¼ ë§ˆë‹ˆë˜ ë¯¸ì…˜ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

    ### ì—­í• 
    - ì½˜í…ì¸  í‰ê°€ ì „ë¬¸ê°€ë¡œì„œ, ë§ˆë‹ˆë˜ ë¯¸ì…˜ì˜ ì§ˆ(ì¼ê´€ì„±Â·ì ì ˆì„±Â·ì°½ì˜ì„±Â·ì‹¤í–‰ê°€ëŠ¥ì„±)ì„ ì ìˆ˜í™”í•©ë‹ˆë‹¤.
    - ê° ê¸°ì¤€ì— ëŒ€í•´ 1ì (ë‚®ìŒ)ë¶€í„° 5ì (ë†’ìŒ) ì‚¬ì´ì—ì„œ í‰ê°€í•´ ì£¼ì„¸ìš”.

    ### ì…ë ¥
    í”„ë¡¬í”„íŠ¸: `{query}`
    ìƒì„±ëœ ë¯¸ì…˜: `{mission}`

    ### ì¶œë ¥ í¬ë§·
    - **ë‹µë³€ì€ ë°˜ë“œì‹œ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ 4ê°œì˜ ìˆ«ì**ë¡œë§Œ êµ¬ì„±í•˜ì„¸ìš”.
    - **ì´ìœ ë„ ì¶œë ¥í•˜ì„¸ìš”.**
    - í˜•ì‹ ì˜ˆì‹œ:

    4,5,4,5  
    ì¼ê´€ì„±: í”„ë¡¬í”„íŠ¸ì˜ ì˜ë„ì™€ ìœ ì‚¬í•œ í‘œí˜„ì„ ì˜ ë”°ëìŒ.  
    ì ì ˆì„±: ë¶€ì ì ˆí•œ í‘œí˜„ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ êµ¬ì„±ë¨.  
    ì°½ì˜ì„±: ê¸°ì¡´ ì•„ì´ë””ì–´ì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë§¥ë½ì€ ìœ ì§€ë¨.  
    ì‹¤í–‰ ê°€ëŠ¥ì„±: ì‹¤ì œ ìƒí™©ì—ì„œ ìˆ˜í–‰í•˜ê¸° ì‰¬ì›€.
    """

    prompt = PromptTemplate.from_template(template)

    eval_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=device,
        max_new_tokens=256,
        temperature=0.4,
        top_p=0.8,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        trust_remote_code=True,
        batch_size=4
    )

    eval_llm = HuggingFacePipeline(pipeline=eval_pipe)

    # LangChain LLMChain: prompt | llm êµ¬ì¡°
    eval_llm_chain = prompt | eval_llm
    return eval_llm_chain

def parse_eval_output(raw_output: str) -> dict:
    # 1. ì ìˆ˜ íŒ¨í„´ ì°¾ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ 4ê°œì˜ ìˆ«ì)
    score_matches = list(re.finditer(r"\b([1-5])\s*,\s*([1-5])\s*,\s*([1-5])\s*,\s*([1-5])\b", raw_output))
    
    if len(score_matches) < 2:
        return {
            "error": "2ê°œ ì´ìƒì˜ ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
            "raw_output": raw_output
        }
    
    # 2. ë‘ ë²ˆì§¸ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ
    match = score_matches[1]
    score1, score2, score3, score4 = map(int, match.groups())
    reason_text = raw_output[match.end():].strip()

    return {
        "ì¼ê´€ì„±": score1,
        "ì ì ˆì„±": score2,
        "ì°½ì˜ì„±": score3,
        "ìˆ˜í–‰ê°€ëŠ¥ì„±": score4,
        "ì´í•©": score1 + score2 + score3 + score4,
        "ì´ìœ ": reason_text
    }

def select_query_node(state: dict) -> dict:
  attempt = state.get("attempt", 0)
  difficulty_idx = state.get("difficulty_idx", 0)
  contents = state.get("contents", [[], [], []])
  sbert_model = state.get("sbert_model")
  user_query = state.get("user_query")
  random_queries = state["random_queries"]
  used_user_query = state.get("used_user_query", False)
  
  def get_avg_embedding(texts):
    if not texts:
      return None
    return np.mean(sbert_model.encode(texts, convert_to_numpy=True), axis=0)
  
  if attempt % 2 == 0 and contents[difficulty_idx]:
    print("ì„ë² ë”© ê¸°ë°˜ ì¿¼ë¦¬ ì‚¬ìš©")
    query_embedding = get_avg_embedding(contents[difficulty_idx])
    query_text = "ë³„ë„ ì—†ìŒ"
  else:
    if user_query and not used_user_query:
      query_text = user_query
      query_embedding = sbert_model.encode(user_query, convert_to_numpy=True)
      used_user_query = True
    else:
      query_text = random.choice([user_query] + random_queries) if user_query else random.choice(random_queries)
      query_embedding = sbert_model.encode(query_text, convert_to_numpy=True)
      
  return {
    **state,
    "query": query_text,
    "query_embedding": query_embedding,
    "used_user_query": used_user_query
  }
  
def rag_node(state: dict) -> dict:
  query_embedding = state["query_embedding"]
  difficulty_idx = state["difficulty_idx"]
  mission_collection = state["mission_collection"]
  difficulty_list = [["ìƒ"], ["ì¤‘"], ["í•˜"]]
  
  results = mission_collection.query(
    query_embeddings=[query_embedding],
    n_results=5,
    where={"ë‚œì´ë„": {"$in": difficulty_list[difficulty_idx]}}
  )
  
  documents = results.get("documents", [[]])[0] if results else []
  rag_context_list = [f"- {doc}" for doc in documents] if documents else ["- (ì˜ˆì‹œ ì—†ìŒ)"]
  rag_context = "\n".join(rag_context_list)
  
  return {
    **state,
    "rag_context": rag_context,
    "rag_context_list": rag_context_list
  }
  
def generate_mission_node(state):
  """
  - rag_context, query, group_descriptionì„ ê¸°ë°˜ìœ¼ë¡œ LangChain LLMChainì„ ì‹¤í–‰
  - PromptTemplateì™€ HuggingFacePipelineìœ¼ë¡œ ë¯¸ì…˜ í…ìŠ¤íŠ¸ ìƒì„±
  - ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ stateì— ì¶”ê°€
  """
  
  difficulty_keywords = {"ìƒ": "ì–´ë ¤ìš´", "ì¤‘": "ë³´í†µ", "í•˜": "ì‰¬ìš´"}
  
  gen_llm_chain = state["gen_llm_chain"]
  query = state["query"]
  rag_context = state["rag_context"]
  group_description = state["group_description"]
  
  input_dict = {
    "rag_context": rag_context,
    "query": query,
    "group_description": group_description,
    "difficulty": difficulty_keywords[state["current_diff"]]
  }
  
  result = gen_llm_chain.invoke(input_dict)
  print(f"Generated mission for difficulty '{state['current_diff']}': {result}")
  
  return {**state, "generated": result}

def postprocess_node(state):
  raw_output = state["generated"]  # rag_context_listì—ì„œ ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
  clean_tool = state["clean_tool"]
  sbert_model = state["sbert_model"]
  hated_collection = state["hated_mission_collection"]
  final_output = state.get("final_output", {})
  
  # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ê·œì‹ ì¶”ì¶œ
  if "ë¯¸ì…˜:" in raw_output:
      text = raw_output.split("ë¯¸ì…˜:")[-1].strip()
  else:
      text = raw_output.strip()

  missions = re.findall(r'(?:ë¯¸ì…˜\s*\d+:)?\s*(ë§ˆë‹ˆë [^\n]*?ê¸°)', text, re.MULTILINE)
  missions = list(dict.fromkeys([m.strip() for m in missions]))  # ì¤‘ë³µ ì œê±°
  print(f"Raw missions extracted: {missions}")
  print(f"Extracted {len(missions)} missions from generated text.")

  # 2. ìœ íš¨ì„± ì²´í¬ ë° í˜ì˜¤ ë¯¸ì…˜ í•„í„°ë§
  cleaned = []
  for m in missions:
      if clean_tool.is_valid_mission(m) and not clean_tool.is_in_hated_collection(sbert_model, m, hated_collection, 200):
          m = re.sub(r'^\s*-\s*', '', m)
          cleaned.append(m)
  print(f"Cleaned {len(cleaned)} valid missions after filtering.")
  
  cleaned = cleaned + state["rag_context_list"][:3]

  # 3. DBSCAN ì¤‘ë³µ ì œê±°
  if cleaned:
      embs = sbert_model.encode(cleaned)
      clustering = DBSCAN(eps=0.2, min_samples=1, metric="cosine", n_jobs=-1).fit(embs)
      unique = {}
      for idx, label in enumerate(clustering.labels_):
          if label not in unique:
              unique[label] = cleaned[idx]
      cleaned = list(unique.values())
  print(f"Deduplicated to {len(cleaned)} missions after clustering.")

  # 4. ê¸°ì¡´ ê²°ê³¼ì™€ ìœ ì‚¬ë„ ë¹„êµí•´ì„œ ì¤‘ë³µ ì œê±°
  deduped = []
  for m in cleaned:
      m_emb = sbert_model.encode(m, convert_to_numpy=True)

      existing = [
          mission[0]
          for mission_list in final_output.values()
          for mission in mission_list
          if isinstance(mission, list) and len(mission) > 1 and isinstance(mission[1], str)
      ]

      if existing:
          existing_embs = sbert_model.encode(existing, convert_to_numpy=True)
          sims = cosine_similarity([m_emb], existing_embs)[0]
          if np.max(sims) < 0.8:
              deduped.append(m)
      else:
          deduped.append(m)

  return {**state, "mid_output": deduped}

def eval_node(state):
    eval_llm_chain = state["eval_llm_chain"]       # LangChain LLMChain ì¸ìŠ¤í„´ìŠ¤
    query = state["query"]                          # í”„ë¡¬í”„íŠ¸ (ìƒì„± ìš”ì²­ ë‚´ìš©)
    result = state["mid_output"]                    # ë¯¸ì…˜ ë¦¬ìŠ¤íŠ¸
    current_diff = state["current_diff"]
    emoji_gen = state["emoji_generator"]
    final_output = state.get("final_output", {})    # ëˆ„ì  ì €ì¥ ë”•ì…”ë„ˆë¦¬
    
    for mission in result:
        input_dict = {
            "query": query,
            "mission": mission
        }
        raw_output = eval_llm_chain.invoke(input_dict)
        print("raw_output", raw_output)

        parsed_result = parse_eval_output(raw_output)
        print("parsed_result", parsed_result)
        
        # í‰ê°€ ê²°ê³¼ ì¶œë ¥
        print(f"Mission '{mission}' í‰ê°€ ì ìˆ˜: {parsed_result.get('ì´í•©')}")
        print(f"Mission '{mission}' í‰ê°€ ê²°ê³¼: {parsed_result.get('ì´ìœ ')}")
        print(f"Mission '{mission}' í‰ê°€ ì„¸ë¶€ì‚¬í•­: "
              f"ì¼ê´€ì„±={parsed_result.get('ì¼ê´€ì„±')}, "
              f"ì ì ˆì„±={parsed_result.get('ì ì ˆì„±')}, "
              f"ì°½ì˜ì„±={parsed_result.get('ì°½ì˜ì„±')}, "
              f"ìˆ˜í–‰ê°€ëŠ¥ì„±={parsed_result.get('ìˆ˜í–‰ê°€ëŠ¥ì„±')}, ")
        
        if parsed_result.get("ì´í•©") < 12:
            continue

        if current_diff not in final_output:
            final_output[current_diff] = []
        
        # ê²°ê³¼ ì €ì¥
        final_output[current_diff].append([
            emoji_gen.add_emojis(mission),
            f"ë§ˆë‹ˆë˜ ë¯¸ì…˜: ğŸ«¢ {mission.strip().split()[-1]}",
            parsed_result.get("ì¼ê´€ì„±", None),
            parsed_result.get("ì ì ˆì„±", None),
            parsed_result.get("ì°½ì˜ì„±", None),
            parsed_result.get("ìˆ˜í–‰ê°€ëŠ¥ì„±", None),
            parsed_result.get("ì´í•©", 0),
            parsed_result.get("ì´ìœ ", None)
        ])
        
    return {**state, "final_output": final_output}

def check_completion_node(state):
    target_counts = state.get("target_counts")
    final_output = state["final_output"]
    current_diff = state["current_diff"]

    is_done = all(
        len(final_output.get(diff, [])) >= target_counts.get(diff, 0)
        for diff in target_counts
    )
    
    partially_done = len(final_output.get(current_diff, [])) >= target_counts.get(current_diff, 0)
    
    need_process = None
    
    if is_done:
      need_process = "end"
    elif partially_done:
      need_process = "update"
    else:
      need_process = "generate"
      
    print(f"Check completion: {need_process} for difficulty '{current_diff}'")
    
    return {**state, "done": need_process}

def update_state_node(state):
    difficulty_order = state["difficulty_order"]
    difficulty_idx = state["difficulty_idx"]
    attempt = state["attempt"]
    done = state["done"]

    # ì‹œë„ ìˆ˜ ì¦ê°€
    attempt += 1

    # ë‚œì´ë„ ë³€ê²½
    if done == "update" and difficulty_idx + 1 < len(difficulty_order):
        difficulty_idx += 1
        current_diff = difficulty_order[difficulty_idx]
    else:
        current_diff = state["current_diff"]
    
    print(f"Attempt {attempt}, Difficulty Index {difficulty_idx}, Current Difficulty: {current_diff}")

    return {
        **state,
        "attempt": attempt,
        "difficulty_idx": difficulty_idx,
        "current_diff": current_diff
    }